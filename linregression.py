# -*- coding: utf-8 -*-
"""linregression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gJUfagz1-kLsxo2CWUmmxsc2O2B0vMgB

## Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("Advertising.csv")

df.head()

"""### Multiple Features (N-Dimensional)"""

# Relationships between features
sns.pairplot(df,diag_kind='kde')

"""## Introducing SciKit Learn

We will work a lot with the scitkit learn library, so get comfortable with its model estimator syntax, as well as exploring its incredibly useful documentation!

---
"""

X = df.drop('sales',axis=1)
y = df['sales']

"""## Train | Test Split

Make sure you have watched the Machine Learning Overview videos on Supervised Learning to understand why we do this step
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

X_train

y_train

X_test

y_test

"""## Creating a Model (Estimator)

#### Import a model class from a model family
"""

from sklearn.linear_model import LinearRegression

"""#### Create an instance of the model with parameters"""

model = LinearRegression()

model.fit(X_train,y_train)

# X_test

# We only pass in test features
# The model predicts its own y hat
# We can then compare these results to the true y test label value
test_predictions = model.predict(X_test)

test_predictions

from sklearn.metrics import mean_absolute_error,mean_squared_error

MAE = mean_absolute_error(y_test,test_predictions)
MSE = mean_squared_error(y_test,test_predictions)
RMSE = np.sqrt(MSE)

MAE

MSE

RMSE

df['sales'].mean()

quartet = pd.read_csv('anscombes_quartet1.csv')

# y = 3.00 + 0.500x
quartet['pred_y'] = 3 + 0.5 * quartet['x']
quartet['residual'] = quartet['y'] - quartet['pred_y']

sns.scatterplot(data=quartet,x='x',y='y')
sns.lineplot(data=quartet,x='x',y='pred_y',color='red')
plt.vlines(quartet['x'],quartet['y'],quartet['y']-quartet['residual'])

sns.kdeplot(quartet['residual'])

sns.scatterplot(data=quartet,x='y',y='residual')
plt.axhline(y=0, color='r', linestyle='--')

"""---"""

quartet = pd.read_csv('anscombes_quartet2.csv')

quartet.columns = ['x','y']

# y = 3.00 + 0.500x
quartet['pred_y'] = 3 + 0.5 * quartet['x']
quartet['residual'] = quartet['y'] - quartet['pred_y']

sns.scatterplot(data=quartet,x='x',y='y')
sns.lineplot(data=quartet,x='x',y='pred_y',color='red')
plt.vlines(quartet['x'],quartet['y'],quartet['y']-quartet['residual'])

sns.kdeplot(quartet['residual'])

sns.scatterplot(data=quartet,x='y',y='residual')
plt.axhline(y=0, color='r', linestyle='--')

quartet = pd.read_csv('anscombes_quartet4.csv')

quartet

# y = 3.00 + 0.500x
quartet['pred_y'] = 3 + 0.5 * quartet['x']

quartet['residual'] = quartet['y'] - quartet['pred_y']

sns.scatterplot(data=quartet,x='x',y='y')
sns.lineplot(data=quartet,x='x',y='pred_y',color='red')
plt.vlines(quartet['x'],quartet['y'],quartet['y']-quartet['residual'])

sns.kdeplot(quartet['residual'])

sns.scatterplot(data=quartet,x='y',y='residual')
plt.axhline(y=0, color='r', linestyle='--')

"""### Plotting Residuals

It's also important to plot out residuals and check for normal distribution, this helps us understand if Linear Regression was a valid model choice.
"""

# Predictions on training and testing sets
# Doing residuals separately will alert us to any issue with the split call
test_predictions = model.predict(X_test)

# If our model was perfect, these would all be zeros
test_res = y_test - test_predictions

sns.scatterplot(x=y_test,y=test_res)
plt.axhline(y=0, color='r', linestyle='--')

len(test_res)

sns.displot(test_res,bins=25,kde=True)

"""Still unsure if normality is a reasonable approximation? We can check against the [normal probability plot.](https://en.wikipedia.org/wiki/Normal_probability_plot)"""

import scipy as sp

"""-----------

## Retraining Model on Full Data


"""

final_model = LinearRegression()

final_model.fit(X,y)

y_hat = final_model.predict(X)

fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(16,6))

axes[0].plot(df['TV'],df['sales'],'o')
axes[0].plot(df['TV'],y_hat,'o',color='red')
axes[0].set_ylabel("Sales")
axes[0].set_title("TV Spend")

axes[1].plot(df['radio'],df['sales'],'o')
axes[1].plot(df['radio'],y_hat,'o',color='red')
axes[1].set_title("Radio Spend")
axes[1].set_ylabel("Sales")

axes[2].plot(df['newspaper'],df['sales'],'o')
axes[2].plot(df['radio'],y_hat,'o',color='red')
axes[2].set_title("Newspaper Spend");
axes[2].set_ylabel("Sales")
plt.tight_layout();

"""### Residuals

Should be normally distributed as discussed in the video.
"""

residuals = y_hat - y

sns.scatterplot(x=y,y=residuals)
plt.axhline(y=0, color='r', linestyle='--')

"""### Coefficients"""

final_model.coef_

coeff_df = pd.DataFrame(final_model.coef_,X.columns,columns=['Coefficient'])
coeff_df

df.corr()

campaign = [[149,22,12]]

final_model.predict(campaign)

"""-----

Saving and Loading a Model
"""

from joblib import dump, load

dump(final_model, 'sales_model.joblib')

loaded_model = load('sales_model.joblib')

loaded_model.predict(campaign)